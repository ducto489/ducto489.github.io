<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> VietConizer, Vietnamese OCR with NVIDIA DALI | Dustin Nguyen </title> <meta name="author" content="Dustin Nguyen"> <meta name="description" content="Using DALI to speedup data processing in OCR"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a26b1b0e2d9c9dd2e5ed17c987a37731"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?9dbe0c76bacd463500b79ed6a86e27ac"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/faviconphoto.jpg?89d3779cde599fb5f5a3b84c36489ca0"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ducto489.github.io/projects/ocr-dali/"> <script src="/assets/js/theme.js?bd888c560287cd675855c7662a167c4a"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?3e7054dc4d3e3dd8f0731a48453e618e"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "VietConizer, Vietnamese OCR with NVIDIA DALI",
            "description": "Using DALI to speedup data processing in OCR",
            "published": "May 14, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Dustin</span> Nguyen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Project <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"> <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>VietConizer, Vietnamese OCR with NVIDIA DALI</h1> <p>Using DALI to speedup data processing in OCR</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#1-introduction">1. Introduction</a> </div> <div> <a href="#2-data-processing-and-details">2. Data Processing and Details</a> </div> <ul> <li> <a href="#character-normalization">Character Normalization</a> </li> <li> <a href="#datasets-used">Datasets Used</a> </li> </ul> <div> <a href="#3-training-strategy">3. Training Strategy</a> </div> <div> <a href="#4-why-nvidia-dali">4. Why NVIDIA DALI?</a> </div> <div> <a href="#5-when-and-how-to-integrate-dali-for-faster-ocr-training">5. When and How to Integrate DALI for Faster OCR Training</a> </div> <div> <a href="#6-practical-integration-and-experimental-results">6. Practical Integration and Experimental Results</a> </div> <ul> <li> <a href="#case-1-high-end-gpu-with-strong-cpu-nvidia-a6000">Case 1 High-End GPU with Strong CPU (NVIDIA A6000)</a> </li> <li> <a href="#case-2-cloud-gpu-nvidia-l4">Case 2 Cloud GPU (NVIDIA L4)</a> </li> <li> <a href="#case-3-mid-range-gpu-with-slow-storage-nvidia-3060-hdd">Case 3 Mid-Range GPU with Slow Storage (NVIDIA 3060 + HDD)</a> </li> </ul> <div> <a href="#7-analysis-and-conclusion">7. Analysis and Conclusion</a> </div> </nav> </d-contents> <p>For a detailed exploration of the code, and methods, you can view this <a href="https://github.com/ducto489/lib_ocr" rel="external nofollow noopener" target="_blank">Github Repo</a>. You can see how we perform inference with our pretrained model in <a href="https://github.com/ducto489/lib_ocr/blob/training/inference/inference.ipynb" rel="external nofollow noopener" target="_blank">notebook</a>.</p> <h1 id="accelerating-ocr-training-with-nvidia-dali-a-practical-guide-and-case-study">Accelerating OCR Training with NVIDIA DALI: A Practical Guide and Case Study</h1> <h2 id="1-introduction">1. Introduction</h2> <p>Training Deep Learning models for Optical Character Recognition (OCR) often involves complex data loading and augmentation pipelines. These preprocessing steps, if not optimized, can become a significant bottleneck, leaving expensive GPU resources underutilized and prolonging training times.</p> <p>This document outlines our approach to leveraging the <strong>NVIDIA Data Loading Library (DALI)</strong> to accelerate the training process for our <strong>ResNet + BiLSTM + Attention</strong> OCR model built with <strong>PyTorch Lightning</strong>. We demonstrate substantial speedups compared to standard data loading methods and showcase the importance of hardware-aware pipeline configuration.</p> <h2 id="2-data-processing-and-details">2. Data Processing and Details</h2> <h3 id="character-normalization">Character Normalization</h3> <p>To create a uniform character set for the OCR model, the following text normalizations are applied to the labels:</p> <table> <thead> <tr> <th style="text-align: left">Original Character(s)</th> <th style="text-align: left">Description</th> <th style="text-align: left">Normalized Character</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"> <code class="language-plaintext highlighter-rouge">“</code>, <code class="language-plaintext highlighter-rouge">”</code> </td> <td style="text-align: left">Smart Quotes</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">"</code></td> </tr> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">’</code></td> <td style="text-align: left">Typographical Apostrophe</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">'</code></td> </tr> <tr> <td style="text-align: left"> <code class="language-plaintext highlighter-rouge">–</code>, <code class="language-plaintext highlighter-rouge">—</code>, <code class="language-plaintext highlighter-rouge">−</code> </td> <td style="text-align: left">Various Dashes</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">-</code></td> </tr> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">…</code></td> <td style="text-align: left">Ellipsis</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">...</code></td> </tr> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">Ð</code></td> <td style="text-align: left">Icelandic Eth (Uppercase)</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">Đ</code></td> </tr> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">ð</code></td> <td style="text-align: left">Icelandic Eth (Lowercase)</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">đ</code></td> </tr> <tr> <td style="text-align: left"> <code class="language-plaintext highlighter-rouge">Ö</code>, <code class="language-plaintext highlighter-rouge">Ō</code> </td> <td style="text-align: left">O with accents</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">O</code></td> </tr> <tr> <td style="text-align: left"> <code class="language-plaintext highlighter-rouge">Ü</code>, <code class="language-plaintext highlighter-rouge">Ū</code> </td> <td style="text-align: left">U with accents</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">U</code></td> </tr> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">Ā</code></td> <td style="text-align: left">A with macron</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">A</code></td> </tr> <tr> <td style="text-align: left"> <code class="language-plaintext highlighter-rouge">ö</code>, <code class="language-plaintext highlighter-rouge">ō</code> </td> <td style="text-align: left">o with accents</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">o</code></td> </tr> <tr> <td style="text-align: left"> <code class="language-plaintext highlighter-rouge">ü</code>, <code class="language-plaintext highlighter-rouge">ū</code> </td> <td style="text-align: left">u with accents</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">u</code></td> </tr> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">ā</code></td> <td style="text-align: left">a with macron</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">a</code></td> </tr> </tbody> </table> <p>This normalization simplifies the vocabulary the model needs to learn.</p> <h3 id="datasets-used">Datasets Used</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dataset-480.webp 480w,/assets/img/dataset-800.webp 800w,/assets/img/dataset-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/dataset.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The training leverages a combined dataset from the following sources:</p> <table> <thead> <tr> <th style="text-align: left">Dataset</th> <th style="text-align: right">Train Samples</th> <th style="text-align: right">Validation Samples</th> <th style="text-align: left">Notes</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><a href="https://github.com/pbcquoc/vietocr" rel="external nofollow noopener" target="_blank">vietocr</a></td> <td style="text-align: right">441,025</td> <td style="text-align: right">110,257</td> <td style="text-align: left">Random word images removed</td> </tr> <tr> <td style="text-align: left"><a href="https://github.com/clovaai/deep-text-recognition-benchmark" rel="external nofollow noopener" target="_blank">Paper (Deep Text Rec. Benchmark)</a></td> <td style="text-align: right">3,287,346</td> <td style="text-align: right">6,992</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left"><a href="https://www.robots.ox.ac.uk/~vgg/data/text/" rel="external nofollow noopener" target="_blank">Synth90k</a></td> <td style="text-align: right">7,224,612</td> <td style="text-align: right">802,731</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left"><a href="https://www.kaggle.com/datasets/hariwh0/cinnamon-ai-handwritten-addresses" rel="external nofollow noopener" target="_blank">Cinnamon AI (Handwritten)</a></td> <td style="text-align: right">1,470</td> <td style="text-align: right">368</td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left"><strong>Combined Total</strong></td> <td style="text-align: right"><strong>~11.0 M</strong></td> <td style="text-align: right"><strong>~0.9 M</strong></td> <td style="text-align: left"> </td> </tr> </tbody> </table> <p><strong>Vietnamese Data:</strong> Please note that Vietnamese samples constitute only <strong>1.76%</strong> (209,120 images) of this combined dataset, from <strong>VietOCR</strong> (207,282) and <strong>Cinnamon AI</strong> (1,838). This reflects the limited availability of public Vietnamese OCR data.</p> <h2 id="3-training-strategy">3. Training Strategy</h2> <p>Due to the significant imbalance in the dataset, with English samples heavily outweighing Vietnamese ones, a direct training approach proved challenging. Training exclusively on the VietOCR dataset resulted in instability. Therefore, we adopted a two-stage training strategy:</p> <ol> <li> <strong>Initial Pre-training:</strong> The model was first trained on the entire combined dataset of approximately 11 million images. This allowed the model to learn general text recognition features from a large and diverse dataset, even though it was predominantly English.</li> <li> <strong>Fine-tuning on VietOCR:</strong> The checkpoint obtained from the initial pre-training phase was then used to fine-tune the model specifically on the VietOCR dataset. This step adapted the learned features to the nuances of Vietnamese text and improved performance on the target language.</li> </ol> <p>The model we release is the one obtained after this fine-tuning process on the VietOCR data. You can see an example of how we perform inference with this model in our <a href="https://github.com/ducto489/lib_ocr/blob/training/inference/inference.ipynb" rel="external nofollow noopener" target="_blank">inference notebook</a>.</p> <h2 id="4-why-nvidia-dali">4. Why NVIDIA DALI?</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Pytorch-Dataloader-480.webp 480w,/assets/img/Pytorch-Dataloader-800.webp 800w,/assets/img/Pytorch-Dataloader-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/Pytorch-Dataloader.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><em><strong>Left (PyTorch DataLoader):</strong> The GPU frequently idles or is underutilized, indicating data bottlenecks. <strong>Right (NVIDIA DALI):</strong> The GPU maintains consistently high utilization. DALI keeps the L4 GPU working hard, reducing wasted cycles and speeding up training.</em></p> <p>DALI is designed specifically to address data pipeline bottlenecks in deep learning workloads. Its key advantages include:</p> <ul> <li> <strong>Pipeline Parallelism:</strong> DALI overlaps data loading, preprocessing, and GPU computation, minimizing idle time for both CPU and GPU.</li> <li> <strong>Optimized Kernels:</strong> It provides highly optimized CPU and GPU implementations (using C++/CUDA) for common data manipulation tasks (decoding, resizing, color augmentation, etc.), executing much faster than typical Python libraries.</li> <li> <strong>Framework Integration:</strong> Seamlessly integrates with popular frameworks like PyTorch, TensorFlow.</li> <li> <strong>Hardware Flexibility:</strong> Allows fine-grained control over whether operations run on the CPU or GPU, enabling optimization for diverse hardware configurations.</li> </ul> <h2 id="5-when-and-how-to-integrate-dali-for-faster-ocr-training">5. When and How to Integrate DALI for Faster OCR Training</h2> <p>Before integrating DALI, it’s crucial to determine if it’s the right tool for your specific bottleneck. As the DALI documentation suggests:</p> <blockquote> <p><strong>“Q: How do I know if DALI can help me?</strong> A: You need to check our docs first and see if DALI operators cover your use case. Then, try to run a couple of iterations of your training with a fixed data source - generating the batch once and reusing it over the test run to see if you can train faster without any data processing. If so, then the data processing is a bottleneck, and in that case, DALI may help.”</p> </blockquote> <p>Following this guidance:</p> <ol> <li> <strong>Verify Operator Coverage:</strong> Check the <a href="https://docs.nvidia.com/deeplearning/dali/user-guide/docs/supported_ops.html" rel="external nofollow noopener" target="_blank">official DALI documentation for supported operators</a>. Ensure DALI provides the necessary functions for your OCR preprocessing pipeline (e.g., image decoding, resizing, padding, rotation, color adjustments, noise addition). Our pipeline utilizes operators like <code class="language-plaintext highlighter-rouge">fn.decoders.image</code>, <code class="language-plaintext highlighter-rouge">fn.resize</code>, <code class="language-plaintext highlighter-rouge">fn.rotate</code>, <code class="language-plaintext highlighter-rouge">fn.color_twist</code>, <code class="language-plaintext highlighter-rouge">fn.warp_affine</code>, and <code class="language-plaintext highlighter-rouge">fn.noise.gaussian</code>, all readily available in DALI.</li> <li> <strong>Identify the Bottleneck:</strong> Perform the suggested test. Modify your existing training loop (without DALI) to load and preprocess <em>one single batch</em> of data, then repeatedly feed this <em>same batch</em> to the model for several training steps. Compare the training speed (e.g., iterations/second or time per step) in this fixed-data scenario to your normal training speed. If the fixed-data training is significantly faster, it strongly indicates that your data loading and preprocessing pipeline is limiting overall performance, and DALI is likely to provide a speedup.</li> </ol> <p><strong>How DALI Delivers Performance Gains:</strong></p> <p>If the above checks suggest DALI is suitable, here’s how it achieves acceleration:</p> <ul> <li> <strong>Executing Operations Efficiently:</strong> DALI replaces standard Python-based processing (like using PIL or OpenCV within a PyTorch <code class="language-plaintext highlighter-rouge">Dataset</code>) with highly optimized C++ and CUDA kernels. This drastically reduces the overhead associated with Python execution for common data manipulation tasks.</li> <li> <strong>Pipeline Parallelism:</strong> DALI constructs a computational graph for your data pipeline. It can then execute different stages of this graph (e.g., reading data, CPU-based augmentation, GPU-based augmentation, transferring data to the GPU) in parallel and asynchronously with the main model training loop running on the GPU. This minimizes idle time where the GPU might be waiting for the next batch.</li> <li> <strong>Hardware-Adaptive Execution:</strong> DALI allows you to specify whether individual operations should run on the CPU (<code class="language-plaintext highlighter-rouge">device='cpu'</code>) or the GPU (<code class="language-plaintext highlighter-rouge">device='gpu'</code>). This is critical for optimization. As our Case Studies show: <ul> <li>On systems with very strong CPUs (like the A6000), performing augmentations on the CPU (<code class="language-plaintext highlighter-rouge">device='cpu'</code>) might be faster, preventing contention on the main training GPU.</li> <li>On systems where the CPU is less powerful relative to the GPU (like the L4), offloading augmentations to the GPU (<code class="language-plaintext highlighter-rouge">device='gpu'</code>) frees up the CPU and leads to better overall throughput. This flexibility allows tuning the pipeline for optimal performance on <em>your specific hardware</em>.</li> </ul> </li> <li> <strong>Optimized Data Reading:</strong> While our example uses <code class="language-plaintext highlighter-rouge">fn.external_source</code> for flexibility with individual files, DALI also offers highly optimized readers for various packed dataset formats (TFRecord, RecordIO, Caffe LMDB, <strong>WebDataset</strong>). If your bottleneck test reveals slow performance even with minimal augmentations, or if dealing with slow storage (like the HDD in Case 3), switching to one of these formats and using the corresponding DALI reader (<code class="language-plaintext highlighter-rouge">fn.readers.*</code>) can dramatically improve data ingestion speed <em>before</em> the processing steps.</li> </ul> <h2 id="6-practical-integration-and-experimental-results">6. Practical Integration and Experimental Results</h2> <p>Integrating DALI into our PyTorch Lightning workflow involves a few key components, demonstrated in our codebase. We define DALI pipelines using the <code class="language-plaintext highlighter-rouge">@pipeline_def</code> decorator, specifying data loading, augmentation, and processing steps using <code class="language-plaintext highlighter-rouge">nvidia.dali.fn</code> operators.</p> <p>For data loading from our custom format (images in a folder, labels in CSV), we utilize <code class="language-plaintext highlighter-rouge">fn.external_source</code> coupled with a Python callable (<code class="language-plaintext highlighter-rouge">ExternalInputCallable</code>) that reads image bytes and encodes labels.</p> <p>To feed data into the PyTorch Lightning training loop, we wrap the DALI pipeline using <code class="language-plaintext highlighter-rouge">DALIGenericIterator</code> which handles batch collation and transfer to the GPU. This setup replaces the standard PyTorch <code class="language-plaintext highlighter-rouge">DataLoader</code>. For those interested in the specific implementation details, please refer to the <code class="language-plaintext highlighter-rouge">DALI_OCRDataModule</code> and associated classes within our project’s source code.</p> <p>We tested our DALI implementation across different hardware setups against a baseline PyTorch DataLoader (<code class="language-plaintext highlighter-rouge">No DALI</code>).</p> <p><strong>(Note:</strong> Dataset size and specifics impact absolute times, but relative speedups are indicative.)</p> <h3 id="case-1-high-end-gpu-with-strong-cpu-nvidia-a6000">Case 1 High-End GPU with Strong CPU (NVIDIA A6000)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ocr_training_time_a6000-480.webp 480w,/assets/img/ocr_training_time_a6000-800.webp 800w,/assets/img/ocr_training_time_a6000-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ocr_training_time_a6000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <em>Observation:</em> High GPU utilization with the standard loader hid a data pipeline bottleneck. DALI running augmentations on the strong <strong>CPU</strong> provided the best speedup (~13%), demonstrating its superior efficiency (optimized kernels, parallelism) over standard Python processing even on capable hardware. DALI on CPU outperformed DALI on GPU here, suggesting CPU execution was more efficient for this specific workload, likely due to lower overhead.</li> </ul> <h3 id="case-2-cloud-gpu-nvidia-l4">Case 2 Cloud GPU (NVIDIA L4)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ocr_training_time_l4-480.webp 480w,/assets/img/ocr_training_time_l4-800.webp 800w,/assets/img/ocr_training_time_l4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ocr_training_time_l4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <em>Observation:</em> On this balanced system, offloading DALI augmentations to the GPU provided the best performance, overcoming the CPU bottleneck observed in the CPU-only DALI configuration.</li> </ul> <h3 id="case-3-mid-range-gpu-with-slow-storage-nvidia-3060--hdd">Case 3 Mid-Range GPU with Slow Storage (NVIDIA 3060 + HDD)</h3> <ul> <li> <strong>DALI (Augmentations on CPU):</strong> Extremely slow, GPU utilization frequently hit 0%. Disk I/O was maxed out.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/HDD-bottleneck-480.webp 480w,/assets/img/HDD-bottleneck-800.webp 800w,/assets/img/HDD-bottleneck-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/HDD-bottleneck.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <em>Observation:</em> DALI optimizes <em>processing</em>, but cannot overcome fundamental I/O limitations from slow storage when reading many small files.</li> <li> <em>Recommendation:</em> Combine DALI with optimized data formats like <strong>WebDataset</strong> using DALI’s dedicated readers (<code class="language-plaintext highlighter-rouge">fn.readers.webdataset</code>) to address the data loading bottleneck first.</li> </ul> <h2 id="7-analysis-and-conclusion">7. Analysis and Conclusion</h2> <p>Our experiments clearly demonstrate that:</p> <ol> <li> <strong>DALI significantly reduces OCR training time</strong> when the data pipeline is a bottleneck (up to ~25% speedup observed on L4).</li> <li>The <strong>optimal placement of DALI operations (CPU vs. GPU) is hardware-dependent.</strong> Tuning the <code class="language-plaintext highlighter-rouge">device</code> parameter for operators is crucial for maximizing performance.</li> <li> <strong>I/O is critical.</strong> On systems with slow storage, optimizing the <em>dataset format and reading method</em> (e.g., using WebDataset with DALI’s readers) is essential <em>before</em> DALI’s processing speedups can be fully realized.</li> </ol> <p>By correctly identifying bottlenecks and leveraging DALI’s optimized kernels, parallelism, and hardware-adaptive execution, we can significantly accelerate OCR model training, enabling faster experimentation and development.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="disqus_thread" style="max-width: 930px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="ducto489",disqus_identifier="/projects/ocr-dali",disqus_title="VietConizer, Vietnamese OCR with NVIDIA DALI";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Dustin Nguyen. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?982c80efc910ea9a6203400dcbd0a3af"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?101922ace415c3a07fdb5a0877aad48f"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-project",title:"Project",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"CV",description:"For full details, please see my CV (PDF) located on the right .",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-vietconizer-vietnamese-ocr-with-nvidia-dali",title:"VietConizer, Vietnamese OCR with NVIDIA DALI",description:"Using DALI to speedup data processing in OCR",section:"Projects",handler:()=>{window.location.href="/projects/ocr-dali/"}},{id:"projects-quantum-random-walk",title:"Quantum Random Walk",description:"The behavior of quantum random walks (QRWs) using the Creutz ladder model",section:"Projects",handler:()=>{window.location.href="/projects/qrw/"}},{id:"projects-train-gpt-2-with-tpu",title:"Train GPT-2 with TPU",description:"Using TPU to speedup and recreate GPT-2 result",section:"Projects",handler:()=>{window.location.href="/projects/train-gpt-2-with-tpu/"}},{id:"projects-reinforcement-learning-play-worlde",title:"Reinforcement Learning play Worlde",description:"Using reinforcement learning to train a bot play wordle",section:"Projects",handler:()=>{window.location.href="/projects/wordle/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%64%75%73%74%6E%6E%30%30@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ducto489","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/dustnn","_blank")}},{id:"socials-facebook",title:"Facebook",section:"Socials",handler:()=>{window.open("https://facebook.com/minhduc.876","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>